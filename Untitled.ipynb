{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Group Assignment\n",
    "\n",
    "Group O-2-8\n",
    "\n",
    "Attacks fall into four main categories:\n",
    "\n",
    "* DOS: denial-of-service, e.g. syn flood;\n",
    "* R2L: unauthorized access from a remote machine, e.g. guessing password;\n",
    "* U2R:  unauthorized access to local superuser (root) privileges, e.g., various buffer overflow attacks;\n",
    "* probing: surveillance and other probing, e.g., port scanning.\n",
    "\n",
    "It is important to note that the test data is not from the same probability distribution as the training data, and it includes specific attack types not in the training data.  This makes the task more realistic.  Some intrusion experts believe that most novel attacks are variants of known attacks and the \"signature\" of known attacks can be sufficient to catch novel variants.  The datasets contain a total of 24 training attack types, with an additional 14 types in the test data only. \n",
    "\n",
    "Agenda\n",
    "1. Load Data\n",
    "2. Inspect Data\n",
    "3. Preprocess Data\n",
    "4. Create A Model\n",
    "5. Make Predictions\n",
    "6. Evaluate Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/software/spark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['SPARK_HOME'])\n",
    "dataset_path=\"/home/ubuntu/challenge_1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Dataset\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Loading\n",
    "\n",
    "Data inspection shows that the data does not have a header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------\n",
    "# Use SparkSession and infer schema, then add a header\n",
    "# ---------\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file://\"+dataset_path+\"full.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=[\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\"dst_bytes\", \\\n",
    "          \"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\", \\\n",
    "          \"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\", \\\n",
    "          \"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\", \\\n",
    "          \"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\\\n",
    "          \"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\", \\\n",
    "          \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\", \\\n",
    "          \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\\\n",
    "          \"dst_host_srv_rerror_rate\"]\n",
    "\n",
    "target=[\"connection\"]\n",
    "\n",
    "fieldnames=features+target\n",
    "\n",
    "rawnames=df.schema.names\n",
    "\n",
    "# Create a small function\n",
    "def updateColNames(df,oldnames,newnames):\n",
    "    for i in range(len(newnames)):\n",
    "        df=df.withColumnRenamed(oldnames[i], newnames[i])\n",
    "    return df\n",
    "\n",
    "df=updateColNames(df,rawnames,fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|attack|\n",
      "+------+\n",
      "|     1|\n",
      "|     1|\n",
      "+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding a Boolean column for attack (=1) or normal (=0)\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn('attack', when(df[\"connection\"] == 'normal', 0).otherwise(1))\n",
    "\n",
    "df.select('attack').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Inspection\n",
    "\n",
    "\n",
    "* How many records do we have?\n",
    "* What is the schema of our data?\n",
    "* Is it numerical , is it categorical?\n",
    "* Visualize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb. of records  : 4898431\n"
     ]
    }
   ],
   "source": [
    "# Print the number of records in the data frame\n",
    "print('Nb. of records  : %d' % df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- protocol_type: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- src_bytes: integer (nullable = true)\n",
      " |-- dst_bytes: integer (nullable = true)\n",
      " |-- land: integer (nullable = true)\n",
      " |-- wrong_fragment: integer (nullable = true)\n",
      " |-- urgent: integer (nullable = true)\n",
      " |-- hot: integer (nullable = true)\n",
      " |-- num_failed_logins: integer (nullable = true)\n",
      " |-- logged_in: integer (nullable = true)\n",
      " |-- num_compromised: integer (nullable = true)\n",
      " |-- root_shell: integer (nullable = true)\n",
      " |-- su_attempted: integer (nullable = true)\n",
      " |-- num_root: integer (nullable = true)\n",
      " |-- num_file_creations: integer (nullable = true)\n",
      " |-- num_shells: integer (nullable = true)\n",
      " |-- num_access_files: integer (nullable = true)\n",
      " |-- num_outbound_cmds: integer (nullable = true)\n",
      " |-- is_host_login: integer (nullable = true)\n",
      " |-- is_guest_login: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- srv_count: integer (nullable = true)\n",
      " |-- serror_rate: double (nullable = true)\n",
      " |-- srv_serror_rate: double (nullable = true)\n",
      " |-- rerror_rate: double (nullable = true)\n",
      " |-- srv_rerror_rate: double (nullable = true)\n",
      " |-- same_srv_rate: double (nullable = true)\n",
      " |-- diff_srv_rate: double (nullable = true)\n",
      " |-- srv_diff_host_rate: double (nullable = true)\n",
      " |-- dst_host_count: integer (nullable = true)\n",
      " |-- dst_host_srv_count: integer (nullable = true)\n",
      " |-- dst_host_same_srv_rate: double (nullable = true)\n",
      " |-- dst_host_diff_srv_rate: double (nullable = true)\n",
      " |-- dst_host_same_src_port_rate: double (nullable = true)\n",
      " |-- dst_host_srv_diff_host_rate: double (nullable = true)\n",
      " |-- dst_host_serror_rate: double (nullable = true)\n",
      " |-- dst_host_srv_serror_rate: double (nullable = true)\n",
      " |-- dst_host_rerror_rate: double (nullable = true)\n",
      " |-- dst_host_srv_rerror_rate: double (nullable = true)\n",
      " |-- connection: string (nullable = true)\n",
      " |-- attack: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|      connection|       avg(duration)|\n",
      "+----------------+--------------------+\n",
      "|      portsweep.|  2329.5862863728034|\n",
      "|    warezclient.|   615.2578431372549|\n",
      "|            spy.|               318.0|\n",
      "|         normal.|  217.82472416710442|\n",
      "|       multihop.|               184.0|\n",
      "|        rootkit.|               100.8|\n",
      "|buffer_overflow.|                91.7|\n",
      "|           perl.|  41.333333333333336|\n",
      "|     loadmodule.|   36.22222222222222|\n",
      "|      ftp_write.|              32.375|\n",
      "|    warezmaster.|               15.05|\n",
      "|           imap.|                 6.0|\n",
      "|            phf.|                 4.5|\n",
      "|   guess_passwd.|  2.7169811320754715|\n",
      "|        ipsweep.|  1.0455091739443956|\n",
      "|           back.|  0.1289151157512483|\n",
      "|          satan.|0.031462371004278886|\n",
      "|        neptune.|1.865642056049484...|\n",
      "|          smurf.|                 0.0|\n",
      "|           land.|                 0.0|\n",
      "|           nmap.|                 0.0|\n",
      "|       teardrop.|                 0.0|\n",
      "|            pod.|                 0.0|\n",
      "+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some stats on numerical features\n",
    "df.groupBy('connection').agg({'duration': 'mean'}).orderBy(\"avg(duration)\", ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|      connection|      avg(src_bytes)|\n",
      "+----------------+--------------------+\n",
      "|      portsweep.|  431708.31182176125|\n",
      "|    warezclient.|    300219.562745098|\n",
      "|           back.|  54156.355878347706|\n",
      "|         normal.|  1477.8462500809535|\n",
      "|            pod.|  1462.6515151515152|\n",
      "|buffer_overflow.|  1400.4333333333334|\n",
      "|          smurf.|    935.773096201199|\n",
      "|       multihop.|  435.14285714285717|\n",
      "|           imap.|   347.5833333333333|\n",
      "|        rootkit.|               294.7|\n",
      "|           perl.|   265.6666666666667|\n",
      "|      ftp_write.|              220.75|\n",
      "|            spy.|               174.5|\n",
      "|     loadmodule.|  151.88888888888889|\n",
      "|   guess_passwd.|  125.33962264150944|\n",
      "|            phf.|                51.0|\n",
      "|    warezmaster.|                49.3|\n",
      "|       teardrop.|                28.0|\n",
      "|           nmap.|  24.424006908462868|\n",
      "|        ipsweep.|  10.436583607082765|\n",
      "|          satan.|  0.9987415051598288|\n",
      "|        neptune.|0.009994244494257088|\n",
      "|           land.|                 0.0|\n",
      "+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some stats on numerical features\n",
    "df.groupBy('connection').agg({'src_bytes': 'mean'}).orderBy(\"avg(src_bytes)\", ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|      connection|      avg(dst_bytes)|\n",
      "+----------------+--------------------+\n",
      "|    warezmaster.|           3922087.7|\n",
      "|       multihop.|   213016.2857142857|\n",
      "|      portsweep.|  202681.31643138384|\n",
      "|           imap.|  54948.666666666664|\n",
      "|           back.|   8232.649568769859|\n",
      "|            phf.|              8127.0|\n",
      "|buffer_overflow.|   6339.833333333333|\n",
      "|      ftp_write.|             5382.25|\n",
      "|        rootkit.|              4276.6|\n",
      "|         normal.|  3234.6501113816985|\n",
      "|     loadmodule.|  3009.8888888888887|\n",
      "|           perl.|              2444.0|\n",
      "|            spy.|              1193.5|\n",
      "|    warezclient.|   719.3176470588236|\n",
      "|   guess_passwd.|  216.18867924528303|\n",
      "|        ipsweep.|   4.394359426328019|\n",
      "|          satan.|   2.127485527309338|\n",
      "|           nmap.| 0.13255613126079446|\n",
      "|       teardrop.| 0.05720122574055159|\n",
      "|        neptune.|8.208825046617731E-4|\n",
      "|            pod.|                 0.0|\n",
      "|          smurf.|                 0.0|\n",
      "|           land.|                 0.0|\n",
      "+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('connection').agg({'dst_bytes': 'mean'}).orderBy(\"avg(dst_bytes)\", ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|      connection|avg(wrong_fragment)|\n",
      "+----------------+-------------------+\n",
      "|       teardrop.| 2.9816138917262514|\n",
      "|            pod.| 0.9810606060606061|\n",
      "|    warezmaster.|                0.0|\n",
      "|          smurf.|                0.0|\n",
      "|           nmap.|                0.0|\n",
      "|   guess_passwd.|                0.0|\n",
      "|           imap.|                0.0|\n",
      "|      ftp_write.|                0.0|\n",
      "|        ipsweep.|                0.0|\n",
      "|          satan.|                0.0|\n",
      "|      portsweep.|                0.0|\n",
      "|           land.|                0.0|\n",
      "|     loadmodule.|                0.0|\n",
      "|        rootkit.|                0.0|\n",
      "|buffer_overflow.|                0.0|\n",
      "|    warezclient.|                0.0|\n",
      "|           perl.|                0.0|\n",
      "|            phf.|                0.0|\n",
      "|       multihop.|                0.0|\n",
      "|        neptune.|                0.0|\n",
      "|           back.|                0.0|\n",
      "|            spy.|                0.0|\n",
      "|         normal.|                0.0|\n",
      "+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some stats on numerical features\n",
    "df.groupBy('connection').agg({'wrong_fragment': 'mean'}).orderBy(\"avg(wrong_fragment)\", ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|      connection|            avg(hot)|\n",
      "+----------------+--------------------+\n",
      "|    warezclient.|   8.049019607843137|\n",
      "|       multihop.|                 3.0|\n",
      "|buffer_overflow.|   2.066666666666667|\n",
      "|            phf.|                 2.0|\n",
      "|           back.|  1.9632319564230594|\n",
      "|   guess_passwd.|  1.0566037735849056|\n",
      "|     loadmodule.|                 1.0|\n",
      "|    warezmaster.|                 0.9|\n",
      "|      ftp_write.|                 0.5|\n",
      "|           imap.|  0.3333333333333333|\n",
      "|        rootkit.|                 0.2|\n",
      "|         normal.| 0.04953530136793379|\n",
      "|      portsweep.|7.682704311917795E-4|\n",
      "|          satan.|6.921721620941354E-4|\n",
      "|          smurf.|                 0.0|\n",
      "|           nmap.|                 0.0|\n",
      "|            pod.|                 0.0|\n",
      "|       teardrop.|                 0.0|\n",
      "|           land.|                 0.0|\n",
      "|        ipsweep.|                 0.0|\n",
      "|           perl.|                 0.0|\n",
      "|        neptune.|                 0.0|\n",
      "|            spy.|                 0.0|\n",
      "+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some stats on numerical features\n",
    "df.groupBy('connection').agg({'hot': 'mean'}).orderBy(\"avg(hot)\", ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+\n",
      "|      connection|avg(num_failed_logins)|\n",
      "+----------------+----------------------+\n",
      "|   guess_passwd.|    1.0566037735849056|\n",
      "|        rootkit.|                   0.1|\n",
      "|          satan.|  2.516989680342311E-4|\n",
      "|         normal.|   9.86861379899484E-5|\n",
      "|    warezmaster.|                   0.0|\n",
      "|          smurf.|                   0.0|\n",
      "|           imap.|                   0.0|\n",
      "|           nmap.|                   0.0|\n",
      "|            pod.|                   0.0|\n",
      "|        ipsweep.|                   0.0|\n",
      "|    warezclient.|                   0.0|\n",
      "|      portsweep.|                   0.0|\n",
      "|           perl.|                   0.0|\n",
      "|           land.|                   0.0|\n",
      "|     loadmodule.|                   0.0|\n",
      "|buffer_overflow.|                   0.0|\n",
      "|      ftp_write.|                   0.0|\n",
      "|       teardrop.|                   0.0|\n",
      "|            phf.|                   0.0|\n",
      "|       multihop.|                   0.0|\n",
      "|        neptune.|                   0.0|\n",
      "|            spy.|                   0.0|\n",
      "|           back.|                   0.0|\n",
      "+----------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some stats on numerical features\n",
    "df.groupBy('connection').agg({'num_failed_logins': 'mean'}).orderBy(\"avg(num_failed_logins)\", ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|         duration|\n",
      "+-------+-----------------+\n",
      "|  count|          4898431|\n",
      "|   mean|48.34243046395876|\n",
      "| stddev|723.3298112546812|\n",
      "|    min|                0|\n",
      "|    max|            58329|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some stats on numerical features\n",
    "df.select(\"duration\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a table for SQL access\n",
    "# df.registerTempTable(\"train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.describe().toPandas().to_csv(\"data_summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the categorical variables\n",
    "\n",
    "* protocol_type\n",
    "* service\n",
    "* flag\n",
    "* connection\n",
    "\n",
    "in term of the number of categories and count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|protocol_type|  count|\n",
      "+-------------+-------+\n",
      "|          tcp|1870598|\n",
      "|          udp| 194288|\n",
      "|         icmp|2833545|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many distict flags we have\n",
    "df.groupby('protocol_type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|  service|count|\n",
      "+---------+-----+\n",
      "|   telnet| 4277|\n",
      "|      ftp| 5214|\n",
      "|     auth| 3382|\n",
      "| iso_tsap| 1052|\n",
      "|   systat| 1056|\n",
      "|     name| 1067|\n",
      "|  sql_net| 1052|\n",
      "|    ntp_u| 3833|\n",
      "|      X11|  135|\n",
      "|    pop_3| 1981|\n",
      "|     ldap| 1041|\n",
      "|  discard| 1059|\n",
      "|   tftp_u|    3|\n",
      "|   Z39_50| 1078|\n",
      "|  daytime| 1056|\n",
      "| domain_u|57782|\n",
      "|    login| 1045|\n",
      "|     smtp|96554|\n",
      "|http_2784|    1|\n",
      "|      mtp| 1076|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many distict services we have\n",
    "df.groupby('service').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|  flag|  count|\n",
      "+------+-------+\n",
      "|RSTOS0|    122|\n",
      "|    S3|     50|\n",
      "|    SF|3744328|\n",
      "|    S0| 869829|\n",
      "|   OTH|     57|\n",
      "|   REJ| 268874|\n",
      "|  RSTO|   5344|\n",
      "|  RSTR|   8094|\n",
      "|    SH|   1040|\n",
      "|    S2|    161|\n",
      "|    S1|    532|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many distict flags we have\n",
    "df.groupby('flag').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|      connection|  count|\n",
      "+----------------+-------+\n",
      "|          smurf.|2807886|\n",
      "|        neptune.|1072017|\n",
      "|         normal.| 972781|\n",
      "|          satan.|  15892|\n",
      "|        ipsweep.|  12481|\n",
      "|      portsweep.|  10413|\n",
      "|           nmap.|   2316|\n",
      "|           back.|   2203|\n",
      "|    warezclient.|   1020|\n",
      "|       teardrop.|    979|\n",
      "|            pod.|    264|\n",
      "|   guess_passwd.|     53|\n",
      "|buffer_overflow.|     30|\n",
      "|           land.|     21|\n",
      "|    warezmaster.|     20|\n",
      "|           imap.|     12|\n",
      "|        rootkit.|     10|\n",
      "|     loadmodule.|      9|\n",
      "|      ftp_write.|      8|\n",
      "|       multihop.|      7|\n",
      "|            phf.|      4|\n",
      "|           perl.|      3|\n",
      "|            spy.|      2|\n",
      "+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('connection').count()\\\n",
    "    .orderBy('count', ascending =False)\\\n",
    "    .show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data Visualizations\n",
    "\n",
    "tbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3a. Create a in-memory DataFrame \n",
    "# df2.registerTempTable(\"network_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# num_features = [\n",
    "    \"duration\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"\n",
    "#]\n",
    "#features = df[num_features]\n",
    "#features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Preprocess Data\n",
    "\n",
    "The data inspetion shows that our dataset contains three categorical variables:\n",
    "\n",
    "* protocol_type\n",
    "* service\n",
    "* flag\n",
    "\n",
    "#### Feature Transformation\n",
    "\n",
    "Since models work over nunmerical values we have to transform  these variables into numeric representation. For this transformation process ( categorical -> numerical ) we will use the following 'functions':\n",
    "\n",
    " 1. **StringIndexer** \n",
    "     https://spark.apache.org/docs/2.2.1/ml-features.html#stringindexer\n",
    "     StringIndexer encodes a string column of labels to a column of label indices.\n",
    "\n",
    " 2. **OneHotEncoder**: \n",
    "     https://spark.apache.org/docs/2.2.1/ml-features.html#onehotencoder\n",
    "     OneHotEncoder maps a column of label indices to a column of binary vectors, with at most a single one-value.\n",
    "     This encoding allows algorithms which expect continuous features, such as Logistic Regression, \n",
    "     to use categorical features.Each categorical column will be indexed using the StringIndexer, \n",
    "     and then converted nto one-hot encoded variables using the One-Hot encoder. \n",
    "     The resulting output has the binary vectors appended to the end of each row.\n",
    "   \n",
    " 3. **VectorAssembler**: \n",
    "     https://spark.apache.org/docs/2.2.1/ml-features.html#vectorassembler\n",
    "     TBW\n",
    "\n",
    " 4. **Pipelines** : \n",
    "    We will have more than 1 'process' or stage in our transforamtion so we use a **Pipeline** \n",
    "    to put stages   together. This greately 'cleans' the code elaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "categoricalColumns = [ \\\n",
    "           \"protocol_type\", \"service\", \"flag\"]\n",
    "\n",
    "stages = [] # stages in our Pipeline\n",
    "for col in categoricalColumns:\n",
    "  \n",
    "  # Category Indexing with StringIndexer\n",
    "  indexer = StringIndexer(inputCol=col, outputCol=col+\"_index\")\n",
    "   \n",
    "  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "  encoder = OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_vector\")\n",
    "  \n",
    "  # Add stages.  These are not run here, but will run all at once later on.\n",
    "  stages += [indexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @ADOLFO: The professor uses this in Lab 4. I don't think we have to do it since our target variable (=attack)\n",
    "#          has already been transformed into a Boolean. What do you think?\n",
    "\n",
    "# Use StringIndexer to encode ALSO our target (income) to label indices.\n",
    "# Convert label into label indices using the StringIndexer\n",
    "# label_stringIdx = StringIndexer(inputCol = \"outcome\", outputCol = \"label\")\n",
    "# stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use StringIndexer to encode ALSO our target (income) to label indices.\n",
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol = \"attack\", outputCol = \"label\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VectorAssembler \n",
    "combine all the feature columns into a single vector column. \n",
    "Vector assembler can be used to combine raw features and features generated by different feature transformers \n",
    "into a single feature vector, in order to train ML models like logistic regression \n",
    "This output will include both the numeric columns and the one-hot encoded binary vector columns in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numericCols_all = [\n",
    "    \"duration\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all numerical features into a vector using VectorAssembler\n",
    "\n",
    "numericCols_model = [\"duration\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\"]\n",
    "\n",
    "assemblerInputs = [ col + \"_vector\" for col in categoricalColumns ] + numericCols_model\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['protocol_type_vector', 'service_vector', 'flag_vector', 'duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent']\n"
     ]
    }
   ],
   "source": [
    "print(assemblerInputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage number 0 protocol_type_index\n",
      "stage number 1 protocol_type_vector\n",
      "stage number 2 service_index\n",
      "stage number 3 service_vector\n",
      "stage number 4 flag_index\n",
      "stage number 5 flag_vector\n",
      "stage number 6 label\n",
      "stage number 7 features\n"
     ]
    }
   ],
   "source": [
    "# Check the stages of our pipeline\n",
    "n=0\n",
    "for s in stages:\n",
    "    print('stage number %d %s' %(n,s.getOutputCol()))\n",
    "    n+=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Model\n",
    " * Create a Pipeline object to group together the stages we defined ( feature transformations )\n",
    " * Create the model\n",
    " * Split data into train and test data\n",
    " * Train the model with train data\n",
    " * Test model predictions with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Run the feature transformations.\n",
    "#  - fit() computes feature statistics as needed.\n",
    "#  - transform() actually transforms the features.\n",
    "\n",
    "transformer = pipeline.fit(df)\n",
    "transformed_df = transformer.transform(df)\n",
    "\n",
    "# Keep relevant columns\n",
    "selection = [\"label\", \"features\", \"duration\", \"src_bytes\"] + assemblerInputs\n",
    "dataset = transformed_df.select(selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training records : 3427798\n",
      "Test records : 1470633 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector, duration: int, src_bytes: int, protocol_type_vector: vector, service_vector: vector, flag_vector: vector, duration: int, src_bytes: int, dst_bytes: int, land: int, wrong_fragment: int, urgent: int]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Randomly split data into training (70%) and test (30%) sets. set seed for reproducibility\n",
    "(train_data, test_data) = dataset.randomSplit([0.7, 0.3], seed = 123)\n",
    "print('Training records : %d' % train_data.count())\n",
    "print('Test records : %d ' % test_data.count())\n",
    "train_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Train model with Training Data\n",
    "model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "# LogisticRegression.transform() will only use the 'features' column.\n",
    "\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# See model's predictions and probabilities of each prediction class\n",
    "# You can select any columns in the above schema to view as well. \n",
    "# For example's sake we will choose age & occupation\n",
    "# selected = predictions.select(\"label\", \"prediction\", \"duration\",\"src_bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability : \n",
    "# Here the probability column specifies the probability that the label is 0 (<=50K/yr) or 1 (>50K/yr)\n",
    "# The algorithm selects (= predicts) the outcome with the highest probability\n",
    "# selected.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics:\n",
    "https://spark.apache.org/docs/2.2.1/mllib-evaluation-metrics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score is : 0.000000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "score = evaluator.evaluate(predictions)\n",
    "print('Score is : %03f' % score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

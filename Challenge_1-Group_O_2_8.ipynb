{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Group Assignment\n",
    "\n",
    "Group O-2-8\n",
    "\n",
    "Attacks fall into four main categories:\n",
    "\n",
    "* DOS: denial-of-service, e.g. syn flood;\n",
    "* R2L: unauthorized access from a remote machine, e.g. guessing password;\n",
    "* U2R:  unauthorized access to local superuser (root) privileges, e.g., various buffer overflow attacks;\n",
    "* probing: surveillance and other probing, e.g., port scanning.\n",
    "\n",
    "It is important to note that the test data is not from the same probability distribution as the training data, and it includes specific attack types not in the training data.  This makes the task more realistic.  Some intrusion experts believe that most novel attacks are variants of known attacks and the \"signature\" of known attacks can be sufficient to catch novel variants.  The datasets contain a total of 24 training attack types, with an additional 14 types in the test data only. \n",
    "\n",
    "Agenda\n",
    "1. Spark-Setup / Load Data\n",
    "2. Inspect Data\n",
    "3. Preprocess Data\n",
    "4. Create A Model\n",
    "5. Make Predictions\n",
    "6. Evaluate Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/software/spark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['SPARK_HOME'])\n",
    "dataset_path=\"/home/ubuntu/challenge_1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Dataset\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Loading\n",
    "\n",
    "Data inspection shows that the data does not have a header. Therefore we are going to use a simple for loop to assign the correct labelling to the columns. Furthermore, we are assignung the variable \"connection\" to the different types of network intrusion attacks. The connection types fall into the following categories:\n",
    "\n",
    "* DOS: denial-of-service, e.g. syn flood;\n",
    "* R2L: unauthorized access from a remote machine, e.g. guessing password;\n",
    "* U2R:  unauthorized access to local superuser (root) privileges, e.g., various buffer overflow attacks;\n",
    "* probing: surveillance and other probing, e.g., port scanning.\n",
    "* normal: no attack was identified\n",
    "\n",
    "#### 1.1.1 Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file://\"+dataset_path+\"full.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = spark.read \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file://\"+dataset_path+\"corrected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=[\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\"dst_bytes\", \\\n",
    "          \"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\", \\\n",
    "          \"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\", \\\n",
    "          \"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\", \\\n",
    "          \"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\\\n",
    "          \"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\", \\\n",
    "          \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\", \\\n",
    "          \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\\\n",
    "          \"dst_host_srv_rerror_rate\"]\n",
    "\n",
    "target=[\"connection\"]\n",
    "\n",
    "fieldnames=features+target\n",
    "\n",
    "rawnames=df.schema.names\n",
    "\n",
    "# Create a small function\n",
    "def updateColNames(df,oldnames,newnames):\n",
    "    for i in range(len(newnames)):\n",
    "        df=df.withColumnRenamed(oldnames[i], newnames[i])\n",
    "    return df\n",
    "\n",
    "df=updateColNames(df,rawnames,fieldnames)\n",
    "\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Creating new attack variable 'label'\n",
    "\n",
    "Regarding the scope of this assignment, there is no need to classify attack types into the correct group (i.e probing or DOS). We simply have to identify whether or not an attack is taking place. Thus, we are creating a new boolean column 'lable':\n",
    "\n",
    "* Assign the value '0' for no attack (=normal)\n",
    "* Assign the value '1' for attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    1|3925650|\n",
      "|    0| 972781|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding a Boolean column for attack (=1) or normal (=0)\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn('label', when(df[\"connection\"] == 'normal.', 0).otherwise(1))\n",
    "\n",
    "df.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Loading Test Data\n",
    "\n",
    "We have to repeat the same process for the test data:\n",
    "\n",
    "* Assign column names\n",
    "* Create new column 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = spark.read \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file://\"+dataset_path+\"corrected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_test=[\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\"dst_bytes\", \\\n",
    "          \"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\", \\\n",
    "          \"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\", \\\n",
    "          \"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\", \\\n",
    "          \"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\\\n",
    "          \"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\", \\\n",
    "          \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\", \\\n",
    "          \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\\\n",
    "          \"dst_host_srv_rerror_rate\"]\n",
    "\n",
    "target_test=[\"connection\"]\n",
    "\n",
    "fieldnames_test=features_test+target_test\n",
    "\n",
    "rawnames_test=df_test.schema.names\n",
    "\n",
    "# Create a small function\n",
    "def updateColNames_test(df_test,oldnames,newnames):\n",
    "    for i in range(len(newnames)):\n",
    "        df_test=df_test.withColumnRenamed(oldnames[i], newnames[i])\n",
    "    return df_test\n",
    "\n",
    "df_test=updateColNames(df_test,rawnames,fieldnames)\n",
    "\n",
    "# df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|    1|250436|\n",
      "|    0| 60593|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding a Boolean column for attack (=1) or normal (=0)\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_test = df_test.withColumn('label', when(df_test[\"connection\"] == 'normal.', 0).otherwise(1))\n",
    "\n",
    "df_test.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Inspection\n",
    "\n",
    "\n",
    "* How many records do we have?\n",
    "* What is the schema of our data?\n",
    "* Is it numerical , is it categorical?\n",
    "* Visualize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb. of records  : 4898431\n"
     ]
    }
   ],
   "source": [
    "# Print the number of records in the data frame\n",
    "print('Nb. of records  : %d' % df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check correlation\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data =[\n",
    "    \"duration\",\"src_bytes\"]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- protocol_type: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- src_bytes: integer (nullable = true)\n",
      " |-- dst_bytes: integer (nullable = true)\n",
      " |-- land: integer (nullable = true)\n",
      " |-- wrong_fragment: integer (nullable = true)\n",
      " |-- urgent: integer (nullable = true)\n",
      " |-- hot: integer (nullable = true)\n",
      " |-- num_failed_logins: integer (nullable = true)\n",
      " |-- logged_in: integer (nullable = true)\n",
      " |-- num_compromised: integer (nullable = true)\n",
      " |-- root_shell: integer (nullable = true)\n",
      " |-- su_attempted: integer (nullable = true)\n",
      " |-- num_root: integer (nullable = true)\n",
      " |-- num_file_creations: integer (nullable = true)\n",
      " |-- num_shells: integer (nullable = true)\n",
      " |-- num_access_files: integer (nullable = true)\n",
      " |-- num_outbound_cmds: integer (nullable = true)\n",
      " |-- is_host_login: integer (nullable = true)\n",
      " |-- is_guest_login: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- srv_count: integer (nullable = true)\n",
      " |-- serror_rate: double (nullable = true)\n",
      " |-- srv_serror_rate: double (nullable = true)\n",
      " |-- rerror_rate: double (nullable = true)\n",
      " |-- srv_rerror_rate: double (nullable = true)\n",
      " |-- same_srv_rate: double (nullable = true)\n",
      " |-- diff_srv_rate: double (nullable = true)\n",
      " |-- srv_diff_host_rate: double (nullable = true)\n",
      " |-- dst_host_count: integer (nullable = true)\n",
      " |-- dst_host_srv_count: integer (nullable = true)\n",
      " |-- dst_host_same_srv_rate: double (nullable = true)\n",
      " |-- dst_host_diff_srv_rate: double (nullable = true)\n",
      " |-- dst_host_same_src_port_rate: double (nullable = true)\n",
      " |-- dst_host_srv_diff_host_rate: double (nullable = true)\n",
      " |-- dst_host_serror_rate: double (nullable = true)\n",
      " |-- dst_host_srv_serror_rate: double (nullable = true)\n",
      " |-- dst_host_rerror_rate: double (nullable = true)\n",
      " |-- dst_host_srv_rerror_rate: double (nullable = true)\n",
      " |-- connection: string (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Exploring numercial variables\n",
    "\n",
    "In total, there are 28 numercial variables in our dataset:\n",
    "\n",
    "* XX continous 22)\n",
    "* XX boolean (6)\n",
    "\n",
    "We are using agg() operations in order to compare means between attack and non-attack networks and receive a couple of insights:\n",
    "\n",
    "* Duration: the mean duration of normal connection is longer\n",
    "* Dst_bytes: the mean number of data bytes from destination to source is 6x greater\n",
    "* Hot: the mean number of 'hot' indiactors is 15x smaller for attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Compare averages of numcerical features between "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|label|     avg(duration)|\n",
      "+-----+------------------+\n",
      "|    0|217.82472416710442|\n",
      "|    1|6.3445052411702525|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some stats on numerical features\n",
    "df.groupBy('label').agg({'duration': 'mean'}).orderBy(\"avg(duration)\", ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|label|    avg(src_bytes)|\n",
      "+-----+------------------+\n",
      "|    1| 1923.030449734439|\n",
      "|    0|1477.8462500809535|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some stats on numerical features\n",
    "df.groupBy('label').agg({'src_bytes': 'mean'}).orderBy(\"avg(src_bytes)\", ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|label|    avg(dst_bytes)|\n",
      "+-----+------------------+\n",
      "|    0|3234.6501113816985|\n",
      "|    1| 563.0735605568505|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('label').agg({'dst_bytes': 'mean'}).orderBy(\"avg(dst_bytes)\", ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label| avg(wrong_fragment)|\n",
      "+-----+--------------------+\n",
      "|    1|8.095474634773859E-4|\n",
      "|    0|                 0.0|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some stats on numerical features\n",
    "df.groupBy('label').agg({'wrong_fragment': 'mean'}).orderBy(\"avg(wrong_fragment)\", ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            avg(hot)|\n",
      "+-----+--------------------+\n",
      "|    0| 0.04953530136793379|\n",
      "|    1|0.003244812960910932|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some stats on numerical features\n",
    "df.groupBy('label').agg({'hot': 'mean'}).orderBy(\"avg(hot)\", ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------+\n",
      "|label|avg(num_failed_logins)|\n",
      "+-----+----------------------+\n",
      "|    0|   9.86861379899484E-5|\n",
      "|    1|  1.553882796479563...|\n",
      "+-----+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some stats on numerical features\n",
    "df.groupBy('label').agg({'num_failed_logins': 'mean'}).orderBy(\"avg(num_failed_logins)\", ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|         duration|\n",
      "+-------+-----------------+\n",
      "|  count|          4898431|\n",
      "|   mean|48.34243046395876|\n",
      "| stddev|723.3298112546812|\n",
      "|    min|                0|\n",
      "|    max|            58329|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some stats on numerical features\n",
    "df.select(\"duration\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a table for SQL access\n",
    "# df.registerTempTable(\"train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.describe().toPandas().to_csv(\"data_summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Exploring the categorical variables\n",
    "\n",
    "Again, we are using grouby() commands to explore the categorical variables and their count().\n",
    "\n",
    "* protocol_type (3 distinct types)\n",
    "* service       (3 distinct types)\n",
    "* flag          (11 distinct types)\n",
    "* connection    (21 distinct types)\n",
    "\n",
    "in term of the number of categories and count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|protocol_type|  count|\n",
      "+-------------+-------+\n",
      "|          tcp|1870598|\n",
      "|          udp| 194288|\n",
      "|         icmp|2833545|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many distict flags we have\n",
    "df.groupby('protocol_type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|  service|count|\n",
      "+---------+-----+\n",
      "|   telnet| 4277|\n",
      "|      ftp| 5214|\n",
      "|     auth| 3382|\n",
      "| iso_tsap| 1052|\n",
      "|   systat| 1056|\n",
      "|     name| 1067|\n",
      "|  sql_net| 1052|\n",
      "|    ntp_u| 3833|\n",
      "|      X11|  135|\n",
      "|    pop_3| 1981|\n",
      "|     ldap| 1041|\n",
      "|  discard| 1059|\n",
      "|   tftp_u|    3|\n",
      "|   Z39_50| 1078|\n",
      "|  daytime| 1056|\n",
      "| domain_u|57782|\n",
      "|    login| 1045|\n",
      "|     smtp|96554|\n",
      "|http_2784|    1|\n",
      "|      mtp| 1076|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many distict services we have\n",
    "df.groupby('service').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|  flag|  count|\n",
      "+------+-------+\n",
      "|RSTOS0|    122|\n",
      "|    S3|     50|\n",
      "|    SF|3744328|\n",
      "|    S0| 869829|\n",
      "|   OTH|     57|\n",
      "|   REJ| 268874|\n",
      "|  RSTO|   5344|\n",
      "|  RSTR|   8094|\n",
      "|    SH|   1040|\n",
      "|    S2|    161|\n",
      "|    S1|    532|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many distict flags we have\n",
    "df.groupby('flag').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|      connection|  count|\n",
      "+----------------+-------+\n",
      "|          smurf.|2807886|\n",
      "|        neptune.|1072017|\n",
      "|         normal.| 972781|\n",
      "|          satan.|  15892|\n",
      "|        ipsweep.|  12481|\n",
      "|      portsweep.|  10413|\n",
      "|           nmap.|   2316|\n",
      "|           back.|   2203|\n",
      "|    warezclient.|   1020|\n",
      "|       teardrop.|    979|\n",
      "|            pod.|    264|\n",
      "|   guess_passwd.|     53|\n",
      "|buffer_overflow.|     30|\n",
      "|           land.|     21|\n",
      "|    warezmaster.|     20|\n",
      "|           imap.|     12|\n",
      "|        rootkit.|     10|\n",
      "|     loadmodule.|      9|\n",
      "|      ftp_write.|      8|\n",
      "|       multihop.|      7|\n",
      "|            phf.|      4|\n",
      "|           perl.|      3|\n",
      "|            spy.|      2|\n",
      "+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('connection').count()\\\n",
    "    .orderBy('count', ascending =False)\\\n",
    "    .show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.3 Exploring data visually (@Adolfo)\n",
    "\n",
    "tbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3a. Create a in-memory DataFrame \n",
    "# df2.registerTempTable(\"network_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data\n",
    "\n",
    "The data inspetion shows that our dataset contains three categorical variables:\n",
    "\n",
    "* protocol_type\n",
    "* service\n",
    "* flag\n",
    "\n",
    "We are going to use StringIndexer, OneHotEncoder, Vector Assembler and a Pipeline to compute feature transformation.\n",
    "\n",
    "* **StringIndexer**: converts a single column to an index column (similar to a factor column in R)\n",
    "* **OneHotEncoder**: One-hot encoding maps a column of label indices to a column of binary vectors, with at most a single one-value. This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features.\n",
    "* **VectorAssembler**: A transformer that combines a given list of columns into a single vector column.\n",
    "* **Pipelines**: Facilitates the creation, tuning, and inspection of practical ML workflows. A Spark Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "categoricalColumns = [ \\\n",
    "           \"protocol_type\", \"service\", \"flag\"]\n",
    "\n",
    "stages = [] # stages in our Pipeline\n",
    "for col in categoricalColumns:\n",
    "  \n",
    "  # Category Indexing with StringIndexer\n",
    "  indexer = StringIndexer(inputCol=col, outputCol=col+\"_index\")\n",
    "   \n",
    "  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "  encoder = OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_vector\")\n",
    "  \n",
    "  # Add stages.  These are not run here, but will run all at once later on.\n",
    "  stages += [indexer, encoder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 VectorAssembler \n",
    "\n",
    "This output will include both the numeric columns and the one-hot encoded binary vector columns in our dataset.\n",
    "\n",
    "We are not going to use all of the numeric features from the dataset. The most important features have been identified while inspecting the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['protocol_type_vector', 'service_vector', 'flag_vector', 'duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent']\n"
     ]
    }
   ],
   "source": [
    "# Transform all numerical features into a vector using VectorAssembler\n",
    "\n",
    "numericCols_model = [\"duration\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\"]\n",
    "\n",
    "assemblerInputs = [ col + \"_vector\" for col in categoricalColumns ] + numericCols_model\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "print(assemblerInputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Checking the stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage number 0 protocol_type_index\n",
      "stage number 1 protocol_type_vector\n",
      "stage number 2 service_index\n",
      "stage number 3 service_vector\n",
      "stage number 4 flag_index\n",
      "stage number 5 flag_vector\n",
      "stage number 6 features\n"
     ]
    }
   ],
   "source": [
    "# Check the stages of our pipeline\n",
    "n=0\n",
    "for s in stages:\n",
    "    print('stage number %d %s' %(n,s.getOutputCol()))\n",
    "    n+=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a Model \n",
    " * Create the model\n",
    " * Split data into train and test data\n",
    " * Train the model with train data\n",
    " * Test model predictions with test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Create Pipleline\n",
    "\n",
    "Group together the stages we defined (feature transformations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "transformer = pipeline.fit(df)\n",
    "transformed_df = transformer.transform(df)\n",
    "\n",
    "# Focus on the relevant columns and define dataset\n",
    "selection = [\"label\", \"features\", \"duration\", \"src_bytes\"] + assemblerInputs     # ASK TOMMY\n",
    "dataset = transformed_df.select(selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Splitting dataset into train and test\n",
    "\n",
    "* 70% train | 30% test\n",
    "* Setting a seed to esnure reproducability of the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training records : 3427798\n",
      "Test records : 1470633 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, duration: int, src_bytes: int, protocol_type_vector: vector, service_vector: vector, flag_vector: vector, duration: int, src_bytes: int, dst_bytes: int, land: int, wrong_fragment: int, urgent: int]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_data, test_data) = dataset.randomSplit([0.7, 0.3], seed = 123)\n",
    "print('Training records : %d' % train_data.count())\n",
    "print('Test records : %d ' % test_data.count())\n",
    "train_data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Create a Logisitc Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Train model with Training Data\n",
    "model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method. Feature have been specified earlier.\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics:\n",
    "\n",
    "Binary classifiers are used to separate the elements of a given dataset into one of two possible groups (e.g. attack or no attack)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score is : 0.999387\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "score = evaluator.evaluate(predictions)\n",
    "print('Score is : %03f' % score )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model Selection\n",
    "\n",
    "** READ THIS : ** : https://spark.apache.org/docs/2.2.1/ml-tuning.html\n",
    "\n",
    "Model selection consists in using data to find the best model or parameters for a given task.\n",
    "\n",
    " * Inspect available parameters for tuning\n",
    " * Use CrossValidation or TrainValidationSplit for parameter tuning\n",
    " * Both requires the following inputs:\n",
    "    *  Estimator: algorithm or Pipeline to tune\n",
    "    *  Set of ParamMaps: parameters to choose from, sometimes called a “parameter grid” to search over\n",
    "    *  Evaluator: metric to measure how well a fitted Model does on held-out test data\n",
    "\n",
    "* At a high level, these model selection tools work as follows:\n",
    "\n",
    "   *  They split the input data into separate training and test datasets.\n",
    "   * For each (training, test) pair, they iterate through the set of ParamMaps:\n",
    "   * For each ParamMap, they fit the Estimator using those parameters, get the fitted Model, and evaluate the Model’s performance using the Evaluator.\n",
    "   \n",
    "*  They finally select the Model produced by the best-performing set of parameters.\n",
    "\n",
    "** An interesting blog on parameter tuning ** :https://www.oreilly.com/ideas/big-datas-biggest-secret-hyperparameter-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regParam: regularization parameter (>= 0). (default: 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParam(\"regParam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParam(\"elasticNetParam\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Parameters Grid for Cross Validation\n",
    "we will create a model for each combination of parameters in the grid specified and evaluate its result\n",
    "\n",
    "We use :\n",
    " 3 regularization param values (regParam)\n",
    " 3 values for maximum nb of iterations\n",
    " 3 values for elasticNetParam\n",
    " The grid will have 3 x 3 x 3 = 27 parameter settings to choose from. \n",
    "\n",
    "\n",
    " Regularization Parameter: \n",
    "\n",
    " (intuitively) is a penalty against complexity. \n",
    " A bigger regParam penalizes \"large\" weight coefficients ,i.e, \n",
    " tries to avoid our model model picking up \"noise,\" or \"deducting a pattern where there is none.\"\n",
    " tries to avoid OVERFITTING\n",
    "\n",
    " ElasticNetParam:\n",
    " read this : https://en.wikipedia.org/wiki/Elastic_net_regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Parameters Grid for Cross Validation\n",
    "# we will create a model for each combination of parameters in the grid specified and evaluate its result\n",
    "#\n",
    "# We use :\n",
    "# 3 regularization param values (regParam)\n",
    "# 3 values for maximum nb of iterations\n",
    "# 3 values for elasticNetParam\n",
    "# The grid will have 3 x 3 x 3 = 27 parameter settings to choose from. \n",
    "\n",
    "\n",
    "# Regularization Parameter: \n",
    "\n",
    "# (intuitively) is a penalty against complexity. \n",
    "# A bigger regParam penalizes \"large\" weight coefficients ,i.e, \n",
    "# tries to avoid our model model picking up \"noise,\" or \"deducting a pattern where there is none.\"\n",
    "# tries to avoid OVERFITTING\n",
    "\n",
    "# ElasticNetParam:\n",
    "# read this : https://en.wikipedia.org/wiki/Elastic_net_regularization\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [1, 5, 10])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 3-fold CrossValidator\n",
    "\n",
    "# numFolds determines the number of train/test dataset pairs used in the cross-validation\n",
    "# The cross validation will compute the  average of the evaluation metrics produced by the n models\n",
    "# by fitting the Estimator on the 3 different (training, test) dataset pairs.\n",
    "\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train_data)\n",
    "# this may take some of time (depends on the amount of models that we're creating and testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model score : 0.997650\n"
     ]
    }
   ],
   "source": [
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(test_data)\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "best_score=evaluator.evaluate(predictions)\n",
    "print('Best model score : %03f' % best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predicting test data\n",
    "\n",
    "We are using the previously created pipleline on the corrected dataset: df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|    1|250436|\n",
      "|    0| 60593|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer_test = pipeline.fit(df_test)\n",
    "transformed_df_test = transformer_test.transform(df_test)\n",
    "\n",
    "# Keep relevant columns\n",
    "selection_test = [\"label\", \"features\", \"duration\", \"src_bytes\"] + assemblerInputs\n",
    "dataset_test = transformed_df_test.select(selection_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions_test = cvModel.transform(dataset_test)        # WE CAN EITHER USE MODEL OR CVMODEL\n",
    "# cvModel uses the best model found from the Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o426.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1715.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1715.0 (TID 7997, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 82, y.size = 87\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$31.apply(LogisticRegression.scala:975)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$31.apply(LogisticRegression.scala:974)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1108)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:904)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:266)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:128)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4$lzycompute(BinaryClassificationMetrics.scala:155)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4(BinaryClassificationMetrics.scala:146)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:223)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:86)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:97)\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:87)\n\tat sun.reflect.GeneratedMethodAccessor161.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 82, y.size = 87\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$31.apply(LogisticRegression.scala:975)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$31.apply(LogisticRegression.scala:974)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1108)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:904)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 15 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-2450d349549c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_score_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best model score : %03f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbest_score_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/software/spark/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/software/spark/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/software/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/software/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/software/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o426.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1715.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1715.0 (TID 7997, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 82, y.size = 87\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$31.apply(LogisticRegression.scala:975)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$31.apply(LogisticRegression.scala:974)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1108)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:904)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:266)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:128)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4$lzycompute(BinaryClassificationMetrics.scala:155)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4(BinaryClassificationMetrics.scala:146)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:223)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:86)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:97)\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:87)\n\tat sun.reflect.GeneratedMethodAccessor161.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 82, y.size = 87\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$31.apply(LogisticRegression.scala:975)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$31.apply(LogisticRegression.scala:974)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1108)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:904)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 15 more\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best model\n",
    "best_score_test = evaluator.evaluate(predictions_test)\n",
    "print('Best model score : %03f' % best_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

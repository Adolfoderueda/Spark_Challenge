{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Group Assignment\n",
    "### Challenge 1 | Network Intrusion Detection\n",
    "\n",
    "Group O-2-8\n",
    "\n",
    "### INDEX\n",
    "1. **Spark Setup and Data Loading**  \n",
    "    1.1 Loading Train Data\n",
    "    1.2 Loading Test Data\n",
    "2. **Data Inspection**  \n",
    "    2.1 Exploring numercial variables  \n",
    "    2.2 Exploring categorical variables  \n",
    "3. **Data Preprocessing**  \n",
    "    3.1 Categorical variable ransformations  \n",
    "    3.2 Creating ML Pipeline  \n",
    "    3.3 Splitting dataset into train and test   \n",
    "4. **Model - Logistic Regression**  \n",
    "    4.1 Model   \n",
    "    4.2 Cross validation  \n",
    "    4.3 Evaluate the model(s)  \n",
    "5. **Predicting df_test**  \n",
    "    5.1 Prediction  \n",
    "    5.2 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/software/spark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['SPARK_HOME'])\n",
    "dataset_path=\"/home/ubuntu/challenge_1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Dataset\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Loading\n",
    "\n",
    "Data inspection shows that the data does not have a header. Therefore we are going to use a simple for loop to assign the correct labelling to the columns. Furthermore, we are assignung the variable \"connection\" to the different types of network intrusion attacks. The connection types fall into the following categories:\n",
    "\n",
    "* DOS: denial-of-service, e.g. syn flood;\n",
    "* R2L: unauthorized access from a remote machine, e.g. guessing password;\n",
    "* U2R:  unauthorized access to local superuser (root) privileges, e.g., various buffer overflow attacks;\n",
    "* probing: surveillance and other probing, e.g., port scanning.\n",
    "* normal: no attack was identified\n",
    "\n",
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file://\"+dataset_path+\"full.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=[\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\"dst_bytes\", \\\n",
    "          \"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\", \\\n",
    "          \"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\", \\\n",
    "          \"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\", \\\n",
    "          \"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\\\n",
    "          \"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\", \\\n",
    "          \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\", \\\n",
    "          \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\\\n",
    "          \"dst_host_srv_rerror_rate\"]\n",
    "\n",
    "target=[\"connection\"]\n",
    "\n",
    "fieldnames=features+target\n",
    "\n",
    "rawnames=df.schema.names\n",
    "\n",
    "# Create a small function\n",
    "def updateColNames(df,oldnames,newnames):\n",
    "    for i in range(len(newnames)):\n",
    "        df=df.withColumnRenamed(oldnames[i], newnames[i])\n",
    "    return df\n",
    "\n",
    "df=updateColNames(df,rawnames,fieldnames)\n",
    "\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating new attack variable 'label'\n",
    "\n",
    "Regarding the scope of this assignment, there is no need to classify attack types into the correct group (i.e probing or DOS). We simply have to identify whether or not an attack is taking place. Thus, we are creating a new boolean column 'lable':\n",
    "\n",
    "* Assign the value '0' for no attack (=normal)\n",
    "* Assign the value '1' for attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    1|3925650|\n",
      "|    0| 972781|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding a Boolean column for attack (=1) or normal (=0)\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn('label', when(df[\"connection\"] == 'normal.', 0).otherwise(1))\n",
    "\n",
    "df.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Loading Test Data\n",
    "\n",
    "We have to repeat the same process for the test data:\n",
    "\n",
    "* Assign column names\n",
    "* Create new column 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = spark.read \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"file://\"+dataset_path+\"corrected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_test=[\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\"dst_bytes\", \\\n",
    "          \"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\", \\\n",
    "          \"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\", \\\n",
    "          \"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\", \\\n",
    "          \"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\\\n",
    "          \"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\", \\\n",
    "          \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\", \\\n",
    "          \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\\\n",
    "          \"dst_host_srv_rerror_rate\"]\n",
    "\n",
    "target_test=[\"connection\"]\n",
    "\n",
    "fieldnames_test=features_test+target_test\n",
    "\n",
    "rawnames_test=df_test.schema.names\n",
    "\n",
    "# Create a small function\n",
    "def updateColNames_test(df_test,oldnames,newnames):\n",
    "    for i in range(len(newnames)):\n",
    "        df_test=df_test.withColumnRenamed(oldnames[i], newnames[i])\n",
    "    return df_test\n",
    "\n",
    "df_test=updateColNames(df_test,rawnames,fieldnames)\n",
    "\n",
    "# df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|    1|250436|\n",
      "|    0| 60593|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding a Boolean column for attack (=1) or normal (=0)\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_test = df_test.withColumn('label', when(df_test[\"connection\"] == 'normal.', 0).otherwise(1))\n",
    "\n",
    "df_test.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Inspection\n",
    "\n",
    "\n",
    "* How many records do we have?\n",
    "* What is the schema of our data?\n",
    "* Is it numerical , is it categorical?\n",
    "* Visualize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb. of records Train : 4898431\n",
      "Nb. of records Test : 311029\n"
     ]
    }
   ],
   "source": [
    "# Print the number of records in the data frame\n",
    "print('Nb. of records Train : %d' % df.count())\n",
    "print('Nb. of records Test : %d' % df_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+--------------------+--------------------+\n",
      "|summary|         duration|         src_bytes|         dst_bytes|      wrong_fragment|   num_failed_logins|\n",
      "+-------+-----------------+------------------+------------------+--------------------+--------------------+\n",
      "|  count|          4898431|           4898431|           4898431|             4898431|             4898431|\n",
      "|   mean|48.34243046395876|1834.6211752293746|1093.6228137132073|6.487791703098401E-4|3.205107921291532E-5|\n",
      "| stddev|723.3298112546812| 941431.0744911365| 645012.3337425214| 0.04285433675493731|0.007299407575927214|\n",
      "|    min|                0|                 0|                 0|                   0|                   0|\n",
      "|    max|            58329|        1379963888|        1309937401|                   3|                   5|\n",
      "+-------+-----------------+------------------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('duration','src_bytes','dst_bytes','wrong_fragment','num_failed_logins').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.1 Exploring numercial variables\n",
    "\n",
    "In total, there are 28 numercial variables in our dataset:\n",
    "\n",
    "* 22 continous\n",
    "* 6 boolean\n",
    "\n",
    "#### Correlation between label and numeric columns\n",
    "\n",
    "We are using the correlation function from Lab 5 to compute the correlation between all numerical variables and the traget variable 'labe'. Find a list of the eight most correlating features below:\n",
    "\n",
    "* 0.76 |\t**count**\n",
    "* 0.65 |\t**dst_host_count**\n",
    "* 0.57 |\t**srv_count**\n",
    "* 0.48 |\t**dst_host_same_src_port_rate**\n",
    "* 0.23 |\t**dst_host_srv_serror_rate**\n",
    "* 0.23 |\t**serror_rate**\n",
    "* 0.22 |\t**srv_serror_rate**\n",
    "* 0.22 |\t**dst_host_serror_rate**\n",
    "\n",
    "\n",
    "Those will be the only numercial features we are going to use for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop categorical column\n",
    "cor_data = df.drop('connection','flag', 'protocol_type', 'service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst = []\n",
    "def computeCorrelation(df,targetColumnName):\n",
    "    from pyspark.ml.stat import Correlation\n",
    "    for col in df.columns:\n",
    "        r=df.stat.corr(col,targetColumnName)\n",
    "        lst.append(r)\n",
    "        #print(\"Pearson correlation : %s %s %f \\n\" %(col,targetColumnName,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeCorrelation(cor_data, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cor_data = pd.DataFrame({\"correlation\" : lst, \"features\" : cor_data.schema.names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_data.sort_values('correlation', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore data with groupBy() operations\n",
    "We are using agg() operations in order to compare means between attack and non-attack networks and receive a couple of insights:\n",
    "\n",
    "* Duration: the mean duration of normal connection is longer\n",
    "* Dst_bytes: the mean number of data bytes from destination to source is 6x greater\n",
    "* Hot: the mean number of 'hot' indiactors is 15x smaller for attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some stats on numerical features\n",
    "df.groupBy('label').agg({'duration': 'mean'}).orderBy(\"avg(duration)\", ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('label').agg({'dst_bytes': 'mean'}).orderBy(\"avg(dst_bytes)\", ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some stats on numerical features\n",
    "df.groupBy('label').agg({'hot': 'mean'}).orderBy(\"avg(hot)\", ascending = False).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Exploring the categorical variables\n",
    "\n",
    "Again, we are using grouby() commands to explore the categorical variables and their count().\n",
    "\n",
    "* protocol_type (3 distinct types)\n",
    "* service       (70 distinct types)\n",
    "* flag          (11 distinct types)\n",
    "* connection    (21 distinct types)\n",
    "\n",
    "in term of the number of categories and count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many distict flags we have\n",
    "df.groupby('protocol_type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many distict services we have\n",
    "df.groupby('service').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many distict flags we have\n",
    "df.groupby('flag').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('connection').count()\\\n",
    "    .orderBy('count', ascending =False)\\\n",
    "    .show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data\n",
    "\n",
    "The data inspetion shows that our dataset contains three categorical variables:\n",
    "\n",
    "* protocol_type\n",
    "* service\n",
    "* flag\n",
    "\n",
    "We are going to use StringIndexer, OneHotEncoder, Vector Assembler and a Pipeline to compute feature transformation.\n",
    "\n",
    "* **StringIndexer**: converts a single column to an index column (similar to a factor column in R)\n",
    "* **OneHotEncoder**: One-hot encoding maps a column of label indices to a column of binary vectors, with at most a single one-value. This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features.\n",
    "* **VectorAssembler**: A transformer that combines a given list of columns into a single vector column.\n",
    "* **Pipelines**: Facilitates the creation, tuning, and inspection of practical ML workflows. A Spark Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Categorical variable ransformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protocol_type COLUMN\n",
    "# Create a StringIndexer\n",
    "protocol_type_indexer = StringIndexer(inputCol=\"protocol_type\", outputCol=\"protocol_type_index\")\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "protocol_type_encoder = OneHotEncoder(inputCol=\"protocol_type_index\", outputCol=\"protocol_type_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# service COLUMN\n",
    "#service_indexer = StringIndexer(inputCol=\"service\", outputCol=\"service_index\", handleInvalid='skip')\n",
    "#service_encoder = OneHotEncoder(inputCol=\"service_index\", outputCol=\"service_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flag COLUMN\n",
    "flag_indexer = StringIndexer(inputCol=\"flag\", outputCol=\"flag_index\")\n",
    "flag_encoder = OneHotEncoder(inputCol=\"flag_index\", outputCol=\"flag_fact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VectorAssembler \n",
    "\n",
    "This output will include both the numeric columns and the one-hot encoded binary vector columns in our dataset.\n",
    "\n",
    "We are not going to use all of the numeric features from the dataset. The most important features have been identified while inspecting the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select features\n",
    "num_cols = [\"count\",\"dst_host_count\",\"srv_count\",\"dst_host_same_src_port_rate\",\"dst_host_srv_serror_rate\",\"serror_rate\", \"srv_serror_rate\",\"dst_host_serror_rate\"]\n",
    "fact_cols = [\"protocol_type_fact\", \"flag_fact\"] # \"service_fact\"\n",
    "assembler_input = num_cols + fact_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols= assembler_input,\n",
    "                                outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform all numerical features into a vector using VectorAssembler\n",
    "\n",
    "#numericCols_model = [\"duration\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\"]\n",
    "\n",
    "#assemblerInputs = [ col + \"_index\" for col in categoricalColumns ] + numericCols_model\n",
    "#assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "#stages += [assembler]\n",
    "\n",
    "#print(assemblerInputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Creating a ML Pipeline\n",
    "\n",
    "MLlib uses Pipeline, which consists of a sequence of PipelineStages (Transformers and Estimators) to be run in a specific order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Make the pipeline\n",
    "pipeline = Pipeline(stages=[protocol_type_indexer, protocol_type_encoder, \\\n",
    "                                flag_indexer, flag_encoder, \\\n",
    "                                vec_assembler]) #service_indexer, service_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = pipeline.fit(df)\n",
    "transformed_df = transformer.transform(df)\n",
    "\n",
    "# Focus on the relevant columns and define dataset\n",
    "selection = [\"label\", \"features\"] # + assemblerInputs     # \"duration\", \"src_bytes\"\n",
    "dataset = transformed_df.select(selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Splitting dataset into train and test\n",
    "\n",
    "* 70% train | 30% test\n",
    "* Setting a seed to esnure reproducability of the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training records : 3427798\n",
      "Test records : 1470633 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_data, test_data) = dataset.randomSplit([0.7, 0.3], seed = 123)\n",
    "print('Training records : %d' % train_data.count())\n",
    "print('Test records : %d ' % test_data.count())\n",
    "train_data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Logistic Regression Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Train model with Training Data\n",
    "model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on test data using the transform() method. Feature have been specified earlier.\n",
    "predictions = model.transform(test_data)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(1.0, 103.0, 1.0, 0.01, 0.01, 1.0, 1.0, 0.01, ...</td>\n",
       "      <td>[-0.227890953592, 0.227890953592]</td>\n",
       "      <td>[0.443272557631, 0.556727442369]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>(2.0, 2.0, 4.0, 0.5, 0.01, 0.5, 0.25, 0.5, 0.0...</td>\n",
       "      <td>[1.37571455751, -1.37571455751]</td>\n",
       "      <td>[0.798301857458, 0.201698142542]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>(2.0, 24.0, 2.0, 0.29, 0.01, 0.5, 0.5, 0.04, 0...</td>\n",
       "      <td>[1.58402809983, -1.58402809983]</td>\n",
       "      <td>[0.829774238881, 0.170225761119]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>(2.0, 2.0, 2.0, 0.5, 0.01, 0.5, 0.5, 0.5, 0.0,...</td>\n",
       "      <td>[8.70391720733, -8.70391720733]</td>\n",
       "      <td>[0.999834092993, 0.000165907006548]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>(2.0, 2.0, 2.0, 0.5, 0.01, 0.5, 0.5, 0.5, 0.0,...</td>\n",
       "      <td>[8.70391720733, -8.70391720733]</td>\n",
       "      <td>[0.999834092993, 0.000165907006548]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           features  \\\n",
       "0      0  (1.0, 103.0, 1.0, 0.01, 0.01, 1.0, 1.0, 0.01, ...   \n",
       "1      0  (2.0, 2.0, 4.0, 0.5, 0.01, 0.5, 0.25, 0.5, 0.0...   \n",
       "2      0  (2.0, 24.0, 2.0, 0.29, 0.01, 0.5, 0.5, 0.04, 0...   \n",
       "3      0  (2.0, 2.0, 2.0, 0.5, 0.01, 0.5, 0.5, 0.5, 0.0,...   \n",
       "4      0  (2.0, 2.0, 2.0, 0.5, 0.01, 0.5, 0.5, 0.5, 0.0,...   \n",
       "\n",
       "                       rawPrediction                          probability  \\\n",
       "0  [-0.227890953592, 0.227890953592]     [0.443272557631, 0.556727442369]   \n",
       "1    [1.37571455751, -1.37571455751]     [0.798301857458, 0.201698142542]   \n",
       "2    [1.58402809983, -1.58402809983]     [0.829774238881, 0.170225761119]   \n",
       "3    [8.70391720733, -8.70391720733]  [0.999834092993, 0.000165907006548]   \n",
       "4    [8.70391720733, -8.70391720733]  [0.999834092993, 0.000165907006548]   \n",
       "\n",
       "   prediction  \n",
       "0         1.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics:\n",
    "\n",
    "Binary classifiers are used to separate the elements of a given dataset into one of two possible groups (e.g. attack or no attack)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score is : 0.999171\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "score = evaluator.evaluate(predictions)\n",
    "print('Score is : %03f' % score )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.2 Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regParam: regularization parameter (>= 0). (default: 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParam(\"regParam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParam(\"elasticNetParam\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters Grid for Cross Validation\n",
    "We will create a model for each combination of parameters in the grid specified and evaluate its result using:\n",
    " * 3 regularization param values (regParam)\n",
    " * 3 values for maximum nb of iterations\n",
    " * 3 values for elasticNetParam\n",
    "\n",
    "Thus, the grid will have 3 x 3 x 3 = 27 parameter settings to choose from. \n",
    "\n",
    "\n",
    "**Regularization Parameter:**\n",
    "\n",
    "* (Intuitively) is a penalty against complexity. \n",
    "* A bigger regParam penalizes \"large\" weight coefficients ,i.e, \n",
    "* Tries to avoid our model model picking up \"noise,\" or \"deducting a pattern where there is none.\"\n",
    "* Tries to avoid OVERFITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [1, 5, 10])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create 3-fold Cross Validaton\n",
    "* numFolds determines the number of train/test dataset pairs used in the cross-validation\n",
    "* The cross validation will compute the  average of the evaluation metrics produced by the n models\n",
    "* by fitting the Estimator on the 3 different (training, test) dataset pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train_data)\n",
    "# this may take some of time (depends on the amount of models that we're creating and testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(test_data)\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "best_score=evaluator.evaluate(predictions)\n",
    "# print('Best model score : %03f' % best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predicting test data\n",
    "\n",
    "We are using the previously created pipleline on the corrected dataset: df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use our previous built pipeline to transform df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer_test = pipeline.fit(df_test)\n",
    "transformed_df_test = transformer_test.transform(df_test)\n",
    "\n",
    "# Keep relevant columns\n",
    "selection_test = [\"label\", \"features\"] #+ assemblerInputs #  \"duration\", \"src_bytes\"\n",
    "dataset_test = transformed_df_test.select(selection_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting using cross-validated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions_test = CVmodel.transform(dataset_test)        # WE CAN EITHER USE MODEL OR CVMODEL\n",
    "# cvModel uses the best model found from the Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluator for df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model score : 0.964408\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best model\n",
    "best_score_test = evaluator.evaluate(predictions_test)\n",
    "print('Best model score : %03f' % best_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
